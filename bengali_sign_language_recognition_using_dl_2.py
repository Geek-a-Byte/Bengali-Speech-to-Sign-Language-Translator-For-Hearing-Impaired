# -*- coding: utf-8 -*-
"""Bengali_Sign_Language_Recognition_Using_DL_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1obERRl7uDMOJMjDz12FDIib41na3o9zA

> https://www.kaggle.com/datasets/muntakimrafi/bengali-sign-language-dataset

## Installing dependencies and Required Imports
"""

# # The Pillow library contains all the basic image processing functionality. You can do image resizing, rotation and transformation.
!pip uninstall Pillow
!pip uninstall PIL

!python -m pip install --upgrade pip
!python -m pip install --upgrade Pillow

# import the new one
import PIL
print(PIL.__version__)

CUDA_LAUNCH_BLOCKING=1

# since the input images are quite large, we will disable the size check
PIL.Image.MAX_IMAGE_PIXELS = None

# PyTorch is an open source machine learning library used for developing and training neural network based deep learning models. 
# It is primarily developed by Facebook's AI research group. PyTorch can be used with Python as well as a C++. 
# Naturally, the Python interface is more polished.

# Torchvision is a library for Computer Vision that goes hand in hand with PyTorch. 
# It has utilities for efficient Image and Video transformations, some commonly used pre-trained models, 
# and some datasets ( torchvision does not come bundled with PyTorch, you will have to install it separately. )

# https://tinyurl.com/25ufu7jk

!pip install torchvision
!pip install torch

"""### **you need to restart the runtime before running this cell, else you will get an error in import torchvision**"""

# Imports here

# cv2 is the module import name for opencv-python, "Unofficial pre-built CPU-only OpenCV packages for Python". 
import cv2

# What is Shutil in Python?
# Shutil module offers high-level operation on a file like a copy, create, and remote operation on the file. 
# It comes under Python's standard utility modules. This module helps in automating the process of copying and removal of files and directories.
import shutil 

# The OS module in Python provides functions for creating and removing a directory (folder), 
# fetching its contents, changing and identifying the current directory, etc. 
# You first need to import the os module to interact with the underlying operating system.
import os

import time
from datetime import datetime
from pytz import timezone  

# An OrderedDict is a dictionary subclass that remembers the order that keys were first inserted. 
# The only difference between dict() and OrderedDict() is that:

# OrderedDict preserves the order in which the keys are inserted. 
# A regular dict doesnâ€™t track the insertion order and iterating 
# it gives the values in an arbitrary order. By contrast, the order the items are inserted is remembered by OrderedDict.

# https://tinyurl.com/y9fk9re3

from collections import OrderedDict

# The main motive is to create a copy of Python object that we can modify the copy without changing the original data. 
# In Python, there are two methods to create copies.

# Shallow Copy
# Deep Copy
# We will use the copy module to create the above copies.

# A shallow copy is a copy of an object that stores the reference of the original elements. 
# It creates the new collection object and then occupying it with reference to the child objects found in the original.
# It makes copies of the nested objects' reference and doesn't create a copy of the nested objects. 
# So if we make any changes to the copy of the object will reflect in the original object. 
# We will use the copy() function to implement it.

# A deep copy is a process where we create a new object and add copy elements recursively. 
# We will use the deecopy() method which present in copy module. 
# The independent copy is created of original object and its entire object.
# https://tinyurl.com/2nryk85q
import copy


import torch

# torch.cuda. empty_cache ()[source] Releases all unoccupied cached memory currently held by the caching allocator 
# so that those can be used in other GPU application and visible in nvidia-smi . 
# empty_cache() doesn't increase the amount of GPU memory available for PyTorch.
torch.cuda.empty_cache()

# The torch. nn import gives us access to some helpful neural network things,
# such as various neural network layer types (things like regular fully-connected layers, convolutional layers (for imagery), recurrent layers...etc).
import torch.nn as nn

# torch.optim is a package implementing various optimization algorithms.
import torch.optim as optim

# The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision.
import torchvision
from torchvision import datasets, models, transforms
# Variables are just wrappers for the tensors so you can now easily auto compute the gradients.

# Think of tensors and variables as same thing. Variables are just wrappers for the tensors so you can now easily auto compute the gradients.
# So if a tensor was batmanâ€¦
# A Variable would be batman but with his utility belt onâ€¦:grin:
# Batman can do the job but if he has his utility belt on heâ€™s got cool gadgets to useðŸ˜‰

# torch tensors are actually the data.
# variables wrap tensors, and construct a chain of operations between the tensors, so that the gradients can flow back.
# so, eg you create variable a, and then add 1 to it to get b. Theres now a link stored between a and b, in the creator property of b. Then when you call .backward() on b, 
# the gradient backpropagates, via the function in b.creator into a.
# tensors dont have the concept of gradients, creator etc. they purely store data, and handle operations on that data, like adding, multiplying and stuff.

# https://tinyurl.com/2r2uz6xr
# https://tinyurl.com/2j6keuvm

from torch.autograd import Variable

# torch.optim.lr_scheduler provides several methods to adjust the learning rate based on the number of epochs.
from torch.optim import lr_scheduler

# the SubsetRandomSampler uses randomly select samples from indices. 
from torch.utils.data.sampler import SubsetRandomSampler

import numpy as np
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("selected: ", device)

"""## Loading and Formatting the dataset

[**dataset link**](https://www.kaggle.com/datasets/muntakimrafi/bengali-sign-language-dataset)
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip "/content/drive/MyDrive/CSE 442/BSL.zip" -d "./BSL/"

"""## Creating Dataloaders

"""

# All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. 
# The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]

# https://tinyurl.com/2ndu68yt
# https://tinyurl.com/2pljaewl

data_transforms = {
    'train': transforms.Compose([
        # Crop the images to be of size (224, 224) and convert them to tensors.
        transforms.Resize((224,224)),
        # This just converts your input image to PyTorch tensor.
        transforms.ToTensor(), 
        # This is just input data scaling and these values (mean and std) must have been precomputed for your dataset. Changing these values is also not advised.
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'test': transforms.Compose([
        # First your input image is resized to be of size (256, 256)
        transforms.Resize((224,224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

data_transforms

# data_dir = '/content/CervicalCancerSplitDataset/content/CervicalCancerSplitDataset'
data_dir ='/content/BSL'

# image bhag kore rakhtesi train and validation er jonno

# {x: "data/hymenoptera_data/"+x for x in ['train', 'val']}
# The results will be:
# {'train': 'data/hymenoptera_data/train', 'val': 'data/hymenoptera_data/val'}

image_datasets = {
    x: datasets.ImageFolder(
        os.path.join(data_dir, x),
        data_transforms[x]
    ) 
    for x in ['train', 'test']
}

for x in ['train', 'test']:
    print(x) # train\nvalid
    print(image_datasets[x])

# Now, we will pass the samplers to our dataloader. Note that shuffle=True cannot be used when you're using the SubsetRandomSampler.
# DataLoader(
#     dataset,
#     batch_size=1,
#     shuffle=False,
#     num_workers=0,
#     collate_fn=None,
#     pin_memory=False,
#  )

# 1. Dataset: The first parameter in the DataLoader class is the dataset. This is where we load the data from.
# 2. Batching the data: batch_size refers to the number of training samples used in one iteration. Usually we split our data into training and testing sets, and we may have different batch sizes for each.
# 3. Shuffling the data: shuffle is another argument passed to the DataLoader class. The argument takes in a Boolean value (True/False). If shuffle is set to True, then all the samples are shuffled and loaded in batches. Otherwise they are sent one-by-one without any shuffling.
# 4. Allowing multi-processing: As deep learning involves training models with a lot of data, running only single processes ends up taking a lot of time. In PyTorch, you can increase the number of processes running simultaneously by allowing multiprocessing with the argument num_workers. This also depends on the batch size, but I wouldnâ€™t set num_workers to the same number because each worker loads a single batch, and returns it only once itâ€™s ready.
# 5. Merging datasets: The collate_fn argument is used if we want to merge datasets. This argument is optional, and mostly used when batches are loaded from map-styled datasets.
# 6. Loading data on CUDA tensors: You can directly load datasets as CUDA tensors using the pin_memory argument. It is an optional parameter that takes in a Boolean value; if set to True, the DataLoader class copies Tensors into CUDA-pinned memory before returning them.

# num_workers=0 means that itâ€™s the main process that does the data loading when needed.
# num_workers=1 means you only have a single worker, so it might be slow.

# https://tinyurl.com/2homvwhv

dataloaders = {
    x: torch.utils.data.DataLoader(
       image_datasets[x], 
       batch_size=32,
       shuffle=True, 
       num_workers=4
    )
    for x in ['train', 'test']
}

# or this syntax
# dataloaders = {
#     'train': torch.utils.data.DataLoader(
#         image_datasets['train'],
#         batch_size=4,
#         shuffle=True,
#         num_workers=0
#     ),
#     'val': torch.utils.data.DataLoader(
#         image_datasets['valid'],
#         batch_size=4,
#         shuffle=True,
#         num_workers=0
#     )
# }
 
dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}
class_names = image_datasets['train'].classes 
len(class_names)

"""## Explore the data"""

def imshow(img, title=None):
    """Imshow for Tensor. ei function tensor e converted image kei abar image hishebe show koracche"""
    
    inp = img.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.imshow(inp)

    if title is not None:
        plt.title(title)
    
    plt.pause(0.001)  # pause a bit so that plots are updated


# dataloaders['train'] is a iterable, passing it to iter() returns an iterator which you can iterate trough. 
# You could separate the two functions to better understand what is happening. i = iter(dataloaders['train']) and then next(i).
# If you're running this interactively in a notebook try running next(i) a few more times. 
# Each time you run next(i) it will return the next batch of size 32 of the iterator until there are no batches left. 

# The DataLoader is a function that iterates through all our available data and returns it in the form of batches. For example, if we have a dataset of 32 images, and we decide to batch the data with a size of 4. Our DataLoader would process the data, and return 8 batches of 4 images each.
# The Dataset class is an abstract class representing the dataset. It allows us to treat the dataset as an object of a class, rather than a set of data and labels. Dataset class returns a pair of [input, label] every time it is called. 

# To access an individual batch from the DataLoader, we first pass the DataLoader object to Pythonâ€™s iter() built-in function, which returns an object representing a stream of data.
# Iterator object allows you to traverse through all the elements of a DataLoader, regardless of its specific implementation. An iterator is an object representing a stream of data. You can create an iterator object by applying the iter() built-in function to an iterable.
# With the stream of data, we can use Python built-in next() function to get the next data element in the stream of data. From this, we are expecting to get a batch of samples.
# We can get the next element in a sequence without keeping the entire dataset in memory.
# You can use an iterator to manually loop over the iterable it or repeated passing of an iterator to the built-in function next() returns successive items in the stream. Once, when you consumed an item from an iterator, itâ€™s gone. When no more data are available a StopIteration exception is raised.

# Get a batch of training data, of size 32 as set in the dataloader
# Every call to the dataset iterator will return batch of images of size batch_size. 
# Hence you will have 32 batches
# each batch has at least 88 images 
# so iter iterates until you exhaust all the 88*32=2816 images.

# https://tinyurl.com/2jafjskv
# https://tinyurl.com/2njaph88
# https://tinyurl.com/2jhnpzjl

dataiter = iter(dataloaders['train'])
inputs, classes = next(dataiter)

# print(len(inputs)) # 32
# print(classes) # tensor([2, 4, 0, 4, 3, 1, 4, 3, 2, 3, 2, 1, 0, 2, 2, 3, 0, 2, 3, 2, 0, 3, 3, 3, 4, 0, 0, 1, 1, 0, 4, 1]) will be same as image tiles, tho changes in every run
# print(sum(1 for _ in dataiter)) # 88

# Make a grid from batch
out = torchvision.utils.make_grid(inputs)

imshow(out, title=[class_names[x] for x in classes])

"""# Train Model"""

len(class_names)

# set timezone
my_timezone = timezone('Asia/Dhaka')

train_Acc, train_Loss, val_Acc, val_Loss = [], [], [], []

def train_model(model, criterion, optimizer, scheduler, num_epochs=10):
    since = time.time()
    train_Acc.clear()
    train_Loss.clear()
    val_Acc.clear()
    val_Loss.clear()
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    for epoch in range(num_epochs):
        print('Epoch {}/{} at {}'.format(epoch, num_epochs - 1, datetime.now(my_timezone).strftime('%I:%M:%S %p (%d %b %Y)')))
        print('-' * 10)

        # Each epoch has a training and validation phase
        for phase in ['train', 'test']:
            if phase == 'train':
                scheduler.step()
                model.train()  # Set model to training mode
            else:
                model.eval()   # Set model to evaluate mode

            running_loss = 0.0
            running_corrects = 0

            # Iterate over data.
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)

                # zero the parameter gradients
                optimizer.zero_grad()

                # forward
                # track history if only in train
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    # backward + optimize only if in training phase
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                # statistics
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]

            print('{} Loss: {:.4f} Acc: {:.4f}'.format(
                phase, epoch_loss, epoch_acc))
            
            if phase == 'train':
              train_Acc.append(epoch_acc)
              train_Loss.append(epoch_loss)
            else:
              val_Acc.append(epoch_acc)
              val_Loss.append(epoch_loss)

            # deep copy the model
            if phase == 'test' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())
                

    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(
        time_elapsed // 60, time_elapsed % 60))
    print('Best val Acc: {:4f}'.format(best_acc))

    # load best model weights
    model.load_state_dict(best_model_wts)
    return model

def visualize_model(model, num_images=6):
    was_training = model.training
    model.eval()
    images_so_far = 0
    fig = plt.figure()

    with torch.no_grad():
        for i, (inputs, labels) in enumerate(dataloaders['test']):
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            for j in range(inputs.size()[0]):
                images_so_far += 1
                ax = plt.subplot(num_images//2, 2, images_so_far)
                ax.axis('off')
                ax.set_title('predicted: {}'.format(class_names[preds[j]]))
                imshow(inputs.cpu().data[j])

                if images_so_far == num_images:
                    model.train(mode=was_training)
                    return
        model.train(mode=was_training)

def plot_auc_acc_loss(epochs):

    Epochs = [i for i in range(epochs)]

    plt.style.use('fivethirtyeight')
    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))
    axes[0].plot(Epochs, train_Loss, 'r', label='Training loss')
    axes[0].plot(Epochs, val_Loss, 'g', label='Validation loss')
    axes[0].set_title('Training and Validation Loss')
    axes[0].set_xlabel('Epochs', fontsize=18)
    axes[0].set_ylabel('Loss', fontsize=18)
    axes[0].legend()

    train_Acc2=torch.FloatTensor(train_Acc).detach().cpu().numpy()
    val_Acc2=torch.FloatTensor(val_Acc).detach().cpu().numpy()
    axes[1].plot(Epochs, train_Acc2, 'r', label='Training Accuracy')
    axes[1].plot(Epochs, val_Acc2, 'g', label='Validation Accuracy')
    axes[1].set_title('Training and Validation Accuracy')
    axes[1].set_xlabel('Epochs', fontsize=18)
    axes[1].set_ylabel('Accuracy', fontsize=18)
    axes[1].legend()


    plt.tight_layout()
    plt.show()

    return Epochs

def measure_test_acc(model_name):
    correct = 0
    total = 0

    predictions_list = []
    labels_list = []
    with torch.no_grad():
        for data in dataloaders['test']:
            images, labels = data
            images, labels = images.to('cuda'), labels.to('cuda')
            labels_list.append(labels) 
            outputs = model_name(images)
            
            _, predicted = torch.max(outputs.data, 1)
            predictions_list.append(predicted)
            
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print('Accuracy on test images: ', 100*(correct/total), '%')
    return predictions_list, labels_list

# Commented out IPython magic to ensure Python compatibility.
from itertools import chain 
from sklearn.metrics import confusion_matrix
from sklearn import metrics
import pandas as pd

from sklearn.metrics import precision_score, recall_score, accuracy_score, precision_recall_fscore_support, f1_score, classification_report, confusion_matrix
import seaborn as sns

def print_classification_report(model_name, predictions_list, labels_list):
    predictions_l = [predictions_list[i].tolist() for i in range(len(predictions_list))]
    labels_l = [labels_list[i].tolist() for i in range(len(labels_list))]

    predictions_l = list(chain.from_iterable(predictions_l))
    labels_l = list(chain.from_iterable(labels_l))

    print(confusion_matrix(labels_l, predictions_l))

    # Build confusion matrix
    cf_matrix = confusion_matrix(predictions_l, labels_l)
    df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1), index = [i for i in class_names],
                        columns = [i for i in class_names])
    plt.figure(figsize = (30,30))
    sns.heatmap(df_cm, annot=True)
    plt.savefig(str(model_name)+'output.png')

    print("Classification report for CNN :\n%s\n"
#           % (metrics.classification_report(labels_l, predictions_l)))
    
    prec_score = precision_score(labels_l,predictions_l, average='weighted') * 100
    rec_score = recall_score(labels_l, predictions_l, average='weighted') * 100
    f1score = f1_score(labels_l, predictions_l, average='weighted') * 100

    print(f'precision score was {prec_score:6.2f}%\nrecall score was {rec_score:6.2f}%')
    print(f'f1 score was {f1score:6.2f}%\n')

"""## densenet121

---


"""

# super simplified
model_densenet = models.densenet121(pretrained=True)

for param in model_densenet.parameters():
  param.requires_grad = True

model_densenet.classifier = nn.Linear(1024,38)
model_densenet = model_densenet.to(device)

import torch.optim as optim

# specify loss function
criterion = nn.CrossEntropyLoss()

# We create an optimizer, in this case, SGD with a learning rate of 0.006 and register all the parameters of the model in the optimizer. Observe that all parameters are being optimized
optimizer = optim.SGD(model_densenet.parameters(), lr=0.006, momentum=0.9)

# Decay LR by a factor of 0.1 every 3 epochs
# lr = 0.006 if epoch < 3
# lr = 0.0006 if 3 <= epoch < 6
# lr = 0.00006 if 6 <= epoch < 9
exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)

import torch
torch.cuda.empty_cache()

model_densenet = train_model(model_densenet, criterion, optimizer, exp_lr_scheduler, num_epochs=5)

visualize_model(model_densenet)

torch.save(model_densenet.state_dict(), "BSL_densenet121.pt")

Epochs = plot_auc_acc_loss(epochs=5)

predictions_list, labels_list = measure_test_acc(model_densenet)
print_classification_report('model_densenet', predictions_list, labels_list)

"""# VGG16"""

# super simplified
import torchvision.models as models
model_vgg = models.vgg16(pretrained=True)

for param in model_vgg.parameters():
  param.requires_grad = True
  
num_ftrs = model_vgg.classifier[0].in_features
model_vgg.classifier = nn.Linear(num_ftrs,38)
model_vgg = model_vgg.to(device)

import torch.optim as optim

# specify loss function
criterion = nn.CrossEntropyLoss()

# We create an optimizer, in this case, SGD with a learning rate of 0.006 and register all the parameters of the model in the optimizer. Observe that all parameters are being optimized
optimizer = optim.SGD(model_vgg.parameters(), lr=0.001, momentum=0.9)

# Decay LR by a factor of 0.1 every 3 epochs
# lr = 0.006 if epoch < 3
# lr = 0.0006 if 3 <= epoch < 6
# lr = 0.00006 if 6 <= epoch < 9
exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)

import torch
torch.cuda.empty_cache()

model_vgg = train_model(model_vgg, criterion, optimizer, exp_lr_scheduler,  num_epochs=5)

visualize_model(model_vgg)

torch.save(model_vgg.state_dict(), "BSL_Vgg16.pt")

Epochs = plot_auc_acc_loss(epochs=5)

predictions_list, labels_list = measure_test_acc(model_vgg)
print_classification_report('model_vgg', predictions_list, labels_list)

"""# Mobilenet_V3_small"""

# super simplified
import torchvision.models as models
model_mobilenet = models.mobilenet_v3_small(pretrained=True)

for param in model_mobilenet.parameters():
  param.requires_grad = True
  
num_ftrs = model_mobilenet.classifier[0].in_features
model_mobilenet.classifier = nn.Linear(num_ftrs,38)
model_mobilenet  = model_mobilenet.to(device)

import torch.optim as optim

# specify loss function
criterion = nn.CrossEntropyLoss()

# We create an optimizer, in this case, SGD with a learning rate of 0.006 and register all the parameters of the model in the optimizer. Observe that all parameters are being optimized
optimizer = optim.SGD(model_mobilenet.parameters(), lr=0.006, momentum=0.9)

# Decay LR by a factor of 0.1 every 3 epochs
# lr = 0.006 if epoch < 3
# lr = 0.0006 if 3 <= epoch < 6
# lr = 0.00006 if 6 <= epoch < 9
exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)

import torch
torch.cuda.empty_cache()

model_mobilenet = train_model(model_mobilenet, criterion, optimizer, exp_lr_scheduler,  num_epochs=5)

visualize_model(model_mobilenet)

torch.save(model_mobilenet.state_dict(), "BSL_mobilenet_v3_small.pt")

Epochs = plot_auc_acc_loss(epochs=5)

predictions_list, labels_list = measure_test_acc(model_mobilenet)
print_classification_report('model_mobilenetv3small',predictions_list, labels_list)

"""# Mobilenet_V2"""

# super simplified
import torchvision.models as models
model_mobilenetv2 = models.mobilenet_v2(pretrained=True)

for param in model_mobilenetv2.parameters():
  param.requires_grad = True
model_mobilenetv2.classifier = nn.Linear(1280,38)
model_mobilenetv2  = model_mobilenetv2.to(device)

import torch.optim as optim

# specify loss function
criterion = nn.CrossEntropyLoss()

# We create an optimizer, in this case, SGD with a learning rate of 0.006 and register all the parameters of the model in the optimizer. Observe that all parameters are being optimized
optimizer = optim.SGD(model_mobilenetv2.parameters(), lr=0.006, momentum=0.9)

# Decay LR by a factor of 0.1 every 3 epochs
# lr = 0.006 if epoch < 3
# lr = 0.0006 if 3 <= epoch < 6
# lr = 0.00006 if 6 <= epoch < 9
exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)

import torch
torch.cuda.empty_cache()

model_mobilenetv2 = train_model(model_mobilenetv2, criterion, optimizer, exp_lr_scheduler,  num_epochs=5)

visualize_model(model_mobilenetv2)

torch.save(model_mobilenetv2.state_dict(), "BSL_mobilenet_v2.pt")

Epochs = plot_auc_acc_loss(epochs=5)

predictions_list, labels_list = measure_test_acc(model_mobilenetv2)
print_classification_report('model_mobilenetv2', predictions_list, labels_list)

"""# Resnet50"""

# super simplified
import torchvision.models as models
model_resnet = models.resnet50(pretrained=True)

for param in model_resnet.parameters():
  param.requires_grad = True

model_resnet.classifier = nn.Linear(1024,38)
model_resnet = model_resnet.to(device)

import torch.optim as optim

# specify loss function
criterion = nn.CrossEntropyLoss()

# We create an optimizer, in this case, SGD with a learning rate of 0.006 and register all the parameters of the model in the optimizer. Observe that all parameters are being optimized
optimizer = optim.SGD(model_resnet.parameters(), lr=0.006, momentum=0.9)

# Decay LR by a factor of 0.1 every 3 epochs
# lr = 0.006 if epoch < 3
# lr = 0.0006 if 3 <= epoch < 6
# lr = 0.00006 if 6 <= epoch < 9
exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)

import torch
torch.cuda.empty_cache()

model_resnet = train_model(model_resnet, criterion, optimizer, exp_lr_scheduler,  num_epochs=5)

visualize_model(model_resnet)

torch.save(model_resnet.state_dict(), "BSL_resnet50.pt")

Epochs = plot_auc_acc_loss(epochs=5)

predictions_list, labels_list = measure_test_acc(model_resnet)
print_classification_report('model_resnet', predictions_list, labels_list)